{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95496219",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to AI Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecff2d6",
   "metadata": {},
   "source": [
    "## Context\n",
    "This blog series focuses on model explainability for supervised learning both for blackbox models such as deep learning models as well as interpretable models such as decision trees. This blog explains motivations for explainability as well as laying out basic definitions. The subsequent set of publications, dubbed *intermediate*, will describe basic explainability methods such as SHAPLY and associated software libraries. The final segment of the present publication ill focus on implementation of model explainability in blackbox domain specific fields such as transformer based architecture for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407fca8",
   "metadata": {},
   "source": [
    "# Reasons for explainability\n",
    "[Burkart, Hurber] identify trust, causality, transferability, informativeness, fairness, accountability, making adjustments, and proxy functionality as the most common reasons for the need for AIX. Let us explore some of these reasons in more details.\n",
    "## Trust\n",
    "One of the major issues of automation, specially the kind of automation that is achieved through probabilistic models, is trust of humans. For instance, we trust a human driver is context aware and makes the right decision when needed most of the time, while many people do have doubts about ability of self-driving cars to make the correct decision at the correct time all the time. The ability to interpret a model helps building both emotional and legal trust in automated decision making.\n",
    "## Causality\n",
    "Most ML inference work out probabilistic correlation of two or more phenomena and have no insight into causal structure of events, saving causal inference [[more information at CleaR conference]] (https://www.cclear.cc/AcceptedPapers). For instance, the causal relation between emergence of birds of spring, such as swift, and arrival of the season has been clear for as long as human have lived in those regions where migrating birds arrive just before spring. People do acknowledge that there is a correlation between arrival of the migrant birds and changing of the season, but I do doubt anyone would consider migrant birds are causing spring to arrive.\n",
    "## Fairness\n",
    "You might want to know why you are denied a loan application or a bank account. Opaque decision making does not help establish fairness. There has been numerous studies that human bias has resulted rejection of applicants whose name indicate certain ethnicity or religion. The same has been shown to be occuring based on postal codes and other demarcation factors. In many the disparity in providing opportunity is not intentional, but rooted in human bias. The algorithms are not immune to the same biases. The ability to explain and interpret how a decision is made, can help improve both data and algorithms to make fairer and more ethical decisions.     \n",
    "\n",
    "## accountability\n",
    "There are legal frameworks in place that require humans to justify their decisions. In Germany, for instance, an employer is obliged to provide a rejected candidate with reasons for rejection should the candidate requests such justification. Energy companies are required to provide justification for trading decisions if they own both production and distribution of energy in the supply chain. Car accidents require investigation so that intent and  blame can be assigned to those involved. There are countless other examples that humans are required to justify their decisions. As algorithms are increasingly becoming autonomous decision makers, their decisions also need to be able to justified in accordance with existing and emerging legal requirements.\n",
    "\n",
    "In addition to criteria for improving quality of decision making by the machine, we can help humans learn from better algorithms. If an algorithm becomes consistently better at humans at making certain decisions, which is the goal of automation through AI, studying the decision process can help humans learn from the algorithm and improve their decision process and thus creating a mutually reinforcing decision improvement feedback loop between man and machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31f413",
   "metadata": {},
   "source": [
    "## Supervised learning: definitions\n",
    "### Model \n",
    "model $h(x)=y$ is a supervised learning model where $x\\in \\mathcal{X} \\subseteq \\mathbb{R}^{d \\times l}$ and $y \\in \\mathcal{Y} \\subseteq \\mathbb{R}^k$. In the case of SML a set of labeled data $\\mathcal {D_{\\tau}} = \\{(x_1, y_1)\\ , \\dots ,(x_n, y_n)\\}$ is used to train the model in order for the model to to be able to map $h$ over unseen data $\\mathcal{X^\\pi}=\\{x^1, \\dots, x^k\\}$ to prediction $\\mathcal{Y}=\\{y^1, \\dots, y^n\\}$. In the simple case of single class classification and regression $l=k=1$.\n",
    "### Blackbox model\n",
    "A blackbox model in our context $b: \\mathcal{X} \\rightarrow \\mathcal{Y}$, $b\\in \\mathcal{B}$, where $\\mathcal{B}$ is the hypothesis space for a deep learning model.\n",
    "### Error\n",
    "To evaluate a model we use an error measure that uses some topological distance mechanism on the output manifold to measure distance of a prediction to an observed value or $\\mathcal{E} = p-o$, where $p$ is a predicted value and $o$ is an observed value. For instance RMSE (Root Mean Square Error) is a type of Euclidean distance that uses the euclidean distance between two points in a multi-dimensional space. $RMSE = \\sqrt{{\\frac{1}{n}} \n",
    "\\sum_{i=1}^n(y_i-x_i)^2 }$.  \n",
    "### Learning\n",
    "Given dataset $\\mathcal{D}$, SML attempts to solve optimization problem: $h^*=\\text{arg} \\min\\limits_{h \\in \\mathcal{H}}\\mathcal{E}(h(x))$. In the case of parameteric models such as a deep learning model where the model parameters are represented as $\\theta$ and optimized parameters as $\\theta^*$, the optimization problem can be formulated as: $\\theta^*=\\text{arg} \\min\\limits_{\\theta}\\mathcal{E}(h(x; \\theta))$.\n",
    "\n",
    "For most models solution to the optimization problem is often not unique. The matter is exasperated for complex models, specially in deep learning domain where we are faced with non-convex optimization and $\\theta^*$ is only an approximation to a acceptable local minima. Other issues such as dataset biases make the matter of interpretability even more uncertain. We, therefore, need to find a solution as to explain why a model has made a decision based on a set of circumstances.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032ccfe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
