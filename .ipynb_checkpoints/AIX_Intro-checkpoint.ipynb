{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95496219",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to AI Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecff2d6",
   "metadata": {},
   "source": [
    "## Context\n",
    "This blog series focuses on model explainability for supervised learning both for blackbox models such as deep learning models as well as interpretable models such as decision trees. This blog explains motivations for explainability as well as laying out basic definitions. The subsequent set of publications, dubbed *intermediate*, will describe basic explainability methods such as SHAPLY and associated software libraries. The final segment of the present publication ill focus on implementation of model explainability in blackbox domain specific fields such as transformer based architecture for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb076e",
   "metadata": {},
   "source": [
    "## What is explainability\n",
    "In its core, model explainability is to make decision process of a model, understandable for humans, regardless of the data representation or its intermediary encodings. For instance for a language model an explanation could be presence of a collection of words with certain proximity to one another. This is for instance a bag of words solution, but we know that attention-based modern neural models are much more accurate than statistical models based on bag of words. An explanation for a transformer -based model can be a bag of word approximation on the inference. We therefore can enjoy accuracy of more complex but opaque model, while explain its behaviour consistently with a simpler and less accurate model. Same logic can be applied on time series models making trading decisions or computer vision models. For instance a human interpretable data representation for computer vision, could be a *super-pixel*, a collection of pixels always appearing together that are recognizable by humans, rather than single pixels that a computer vision model might be focusing on. this would be the case with masks in case of convolutional neural networks that when visualized could represent a schematic image of an eye or a nose.      \n",
    "<figure>\n",
    "    <img src='cnn.jpeg' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 1: visual representation of a CNN model for face detection</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407fca8",
   "metadata": {},
   "source": [
    "## Reasons for explainability\n",
    "[Burkart, Hurber] identify trust, causality, transferability, informativeness, fairness, accountability, making adjustments, and proxy functionality as the most common reasons for the need for AIX. Let us explore some of these reasons in more details. \n",
    "### Trust\n",
    "One of the major issues of automation, specially the kind of automation that is achieved through probabilistic models, is trust of humans. For instance, we trust a human driver is context aware and makes the right decision when needed most of the time, while many people do have doubts about ability of self-driving cars to make the correct decision at the correct time all the time. The ability to interpret a model helps building both emotional and legal trust in automated decision making. There are two types of trust that someone who is affected by an inference needs to have on an AI-based system: trust in a specific prediction, which culminates in global explainability and trust the model as a whole will make the right decisions. This sort of trust is achieved through local explainability. Both these concepts are explained later in this blog.\n",
    "### Causality\n",
    "Most ML inference work out probabilistic correlation of two or more phenomena and have no insight into causal structure of events, saving causal inference [[more information at CleaR conference]] (https://www.cclear.cc/AcceptedPapers). For instance, the causal relation between emergence of birds of spring, such as swift, and arrival of the season has been clear for as long as human have lived in those regions where migrating birds arrive just before spring. People do acknowledge that there is a correlation between arrival of the migrant birds and changing of the season, but I do doubt anyone would consider migrant birds are causing spring to arrive.\n",
    "### Fairness\n",
    "You might want to know why you are denied a loan application or a bank account. Opaque decision making does not help establish fairness. There has been numerous studies that human bias has resulted rejection of applicants whose name indicate certain ethnicity or religion. The same has been shown to be occuring based on postal codes and other demarcation factors. In many the disparity in providing opportunity is not intentional, but rooted in human bias. The algorithms are not immune to the same biases. The ability to explain and interpret how a decision is made, can help improve both data and algorithms to make fairer and more ethical decisions.     \n",
    "\n",
    "### accountability\n",
    "There are legal frameworks in place that require humans to justify their decisions. In Germany, for instance, an employer is obliged to provide a rejected candidate with reasons for rejection should the candidate requests such justification. Energy companies are required to provide justification for trading decisions if they own both production and distribution of energy in the supply chain. Car accidents require investigation so that intent and  blame can be assigned to those involved. There are countless other examples that humans are required to justify their decisions. As algorithms are increasingly becoming autonomous decision makers, their decisions also need to be able to justified in accordance with existing and emerging legal requirements.\n",
    "\n",
    "In addition to criteria for improving quality of decision making by the machine, we can help humans learn from better algorithms. If an algorithm becomes consistently better at humans at making certain decisions, which is the goal of automation through AI, studying the decision process can help humans learn from the algorithm and improve their decision process and thus creating a mutually reinforcing decision improvement feedback loop between man and machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31f413",
   "metadata": {},
   "source": [
    "## Supervised learning: definitions\n",
    "### Model \n",
    "model $h(x)=y$ is a supervised learning model where $x\\in \\mathcal{X} \\subseteq \\mathbb{R}^{d \\times l}$ and $y \\in \\mathcal{Y} \\subseteq \\mathbb{R}^k$. In the case of SML a set of labeled data $\\mathcal {D_{\\tau}} = \\{(x_1, y_1)\\ , \\dots ,(x_n, y_n)\\}$ is used to train the model in order for the model to to be able to map $h$ over unseen data $\\mathcal{X^\\pi}=\\{x^1, \\dots, x^k\\}$ to prediction $\\mathcal{Y}=\\{y^1, \\dots, y^n\\}$. In the simple case of single class classification and regression $l=k=1$.\n",
    "### Blackbox model\n",
    "A blackbox model in our context $b: \\mathcal{X} \\rightarrow \\mathcal{Y}$, $b\\in \\mathcal{B}$, where $\\mathcal{B}$ is the hypothesis space for a deep learning model.\n",
    "### Error\n",
    "To evaluate a model we use an error measure that uses some topological distance mechanism on the output manifold to measure distance of a prediction to an observed value or $\\mathcal{E} = p-o$, where $p$ is a predicted value and $o$ is an observed value. For instance RMSE (Root Mean Square Error) is a type of Euclidean distance that uses the euclidean distance between two points in a multi-dimensional space. $RMSE = \\sqrt{{\\frac{1}{n}} \n",
    "\\sum_{i=1}^n(y_i-x_i)^2 }$.  \n",
    "### Learning\n",
    "Given dataset $\\mathcal{D}$, SML attempts to solve optimization problem: \n",
    "$$\n",
    "h^*=\\text{arg} \\min\\limits_{h \\in \\mathcal{H}}\\mathcal{E}(h(x))\n",
    "$$. \n",
    "\n",
    "In the case of parameteric models such as a deep learning model where the model parameters are represented as $\\theta$ and optimized parameters as $\\theta^*$, the optimization problem can be formulated as: \n",
    "\n",
    "$$\n",
    "\\theta^*=\\text{arg} \\min\\limits_{\\theta}\\mathcal{E}(h(x; \\theta))\n",
    "$$.\n",
    "\n",
    "For most models solution to the optimization problem is often not unique. The matter is exasperated for complex models, specially in deep learning domain where we are faced with non-convex optimization and $\\theta^*$ is only an approximation to a acceptable local minima. Other issues such as dataset biases make the matter of interpretability even more uncertain. We, therefore, need to find a solution as to explain why a model has made a decision based on a set of circumstances.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032ccfe",
   "metadata": {},
   "source": [
    "## Explainability Approaches\n",
    "In the context of this blog, there are in general two kinds of explainability, local and global. Local explainability, aka instance explainability, deals with explaining how a decision was made for a single input data $x \\in \\mathcal{D}$. This means that the explanation is valid only for $x$ and its close vicinity. Global, or model, explanation approach, generate explanations for how a model in general arrives at a decision on a dataset $\\mathcal{D}$. \n",
    "**Example**: An face detection system has recognized a person to be a person of interest. Local explainability should be clarify as why that person was matched against a known face. Global explainability, should be able to lay out how the face detection works. One of simplest ways is the projection of internal state of nodes in an image recognition model.\n",
    "\n",
    "*The task of model explanation aims to generate a human understandable interpretation **explanator** for the learning process by extending or modifying the learning process described in the previous section*\n",
    "\n",
    "Please bear in mind that explanators differ from predictors as the former always rely on the latter to perform the explanation task.\n",
    "\n",
    "Table 1 provided a comprehensive classification of explainability approaches.\n",
    "\n",
    "| ExplainabilityApproach | Description                                                                  |\n",
    "|:---                     |:---                                                                            |\n",
    "| ante-hoc               | Explainability is built into the model.                                      |\n",
    "| pos-hoc                | Explainability is created after model creation.                                                            |\n",
    "| instance/local         | Explainability is is only applicable to a single instance of data and its close vicinity.                  |\n",
    "| agnostic               | Explainability is independnt of the model itself and is applicable to many or all models.                  |\n",
    "| data independent       | Explainability mechanism works without additional data and is applicable to many or all relevant datasets. |\n",
    "| data dependent         | Explainability requires data.                                                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c725246",
   "metadata": {},
   "source": [
    "## Model Explanators\n",
    " model explanator takes a model $\\mathcal{x}[input] \\implies \\mathcal{Y}[predictions]$ and a specific labeled dataset as an input and creates and explanation belonging to the set $\\mathcal{E}[explanations]$. More formally:\n",
    " $$\n",
    " e: \\rightarrow (\\mathcal{X} \\rightarrow \\mathcal{Y}) \\times (\\mathcal{X}\\times \\mathcal{Y}) \\rightarrow \\mathcal{E}\n",
    " $$\n",
    " As mentioned in the previous sections, there are two approached to explanation: global and local. In the case of global explanations, explanator $e$ take a model $b$ and a dataset $\\mathcal{D'}$ or $e(b, \\mathcal{D'}$. Local implementation, takes a model $b$ and an instance from the dataset $(x,y) \\in \\mathcal{D'}$ as input. This is consistent with the intuitive definition that global explanators explain a model's inferences over a specific dataset and local explantors provide interpretation for a specific instance of data belonging to a dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f8c3c",
   "metadata": {},
   "source": [
    "## Characteristics of Model Explanators\n",
    "**Interpretability** Interpretability in essence is quantification of understanding of how input values affect one another. For instance how income and the number of dependent children can affect approval score for a loan. It is essential to factor in human limitations as the goal of interpretability to make decisions of a model understandable to humans. An explanation with hundreds of dimensions is basically useless of an interpretation for humans. reducing dimensionality to top 5 or 10 important factors that most significantly affect decisions made by the model is the valid approach. \n",
    "\n",
    "**Local Fidelity:** A surrogate model is an approximation to the original model. The surrogate models tend to be less accurate and indeed not factoring in all the parameters used by the inference model. The explanator still needs to be locally faithful at the instance that is being explained and at its immediate neighborhood. Importance of local fidelity is that local decisions might be influenced by a different set of parameters that differ from a decision to another. Focusing only on global parameters might not be able to explain individual decisions, at least based on small enough set of parameters that are human interpretable.\n",
    "\n",
    "**Model Agnostic:**\n",
    "    An explainer should be able to be trained to explain any model given a dataset and black-box model. This is the same criteria that is required for models themselves to be good at generalization. \n",
    "    \n",
    "**Global Prespective:** Apart from trusting a specific local decision, it is also important to trust the model itself to be making the right decisions most of the time. This is similar to trusting decision making of a human, even though on occasions the human might make sub-optimal decisions. Also making good decisions on occasions, does not make a human a generally good decision maker. So for an explainer to be trusted it has to make good local decisions most of the time; thus incorporating local fidelity and global perspective.s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb341e4",
   "metadata": {},
   "source": [
    "## Post-hoc model explanation for black-box modeles via surrogate models\n",
    "As the name suggests, a black box model is a model, whose innerworkings are opaque to human understanding, such as deep neural networks. This is not a new sort of recognized problems. Even in the early days of neural networks criticism was levied at opaqueness of logic of neural networks through information constituency argument. One way to explain black-box models' inference is to develop a surrogate model that is a white box model capable of explaining decisions that the black-box models make. This way we can benefit from internal complexity of a black-box model such as a ANN, while use a simple model that consistently explains the inferences the more complex model has created. Do bear in mind that justification for effectiveness of neural network is based on *universal approximation theorem*, which in turn proposes to solve complex non-linear problems using approximation to a function that behaves similarly to the problem that we want to solve. \n",
    "\n",
    "Figure 2 shows a simplified version of a simple approximation (blue graph) that can interpret the more complicated inference graph (orange one). This is foundation of machine learning in general. If the distance between predictions and observed is within a certain range we can justify the models' prediction posthoc.\n",
    "\n",
    "<figure>\n",
    "    <img src='approximation.png' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 2: Simple approximation. The complicated orange graph can be approximated and explained by the simpler blue graph </figcaption>\n",
    "</figure>\n",
    "\n",
    "More formally, finding a surrogate model results in solving the problem that is formulated below. In simpler terms, explainability though surrogate models is the process of fitting an explainable model $w$ to make predictions where the average distance between the the outcome of the surrogate model and predictions of the black-box model is bounded:\n",
    "$$\n",
    "w^* = \\text{arg} \\min\\limits_{x \\in \\mathcal{\\chi}} \\frac{1}{|\\mathcal{X}|} \\sum_{x \\in \\mathcal{X}}S(w(x), b(x))\n",
    "$$\n",
    "$S$ is called the ***fidelity score*** and is a measure of how well the white-box surrogate model $w$ approximates the black-box model $b$. The smaller the value for $S$ is, the better the approximate is.\n",
    "\n",
    "We can see that the in figure 2, the fidelity score is twice as high as the one in Figure 1, and hence the approximation is less accurate. \n",
    "\n",
    "<figure>\n",
    "    <img src='approximation1.png' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 3: Even though the orange graph still represents an approximation of the blue graph, it is not as good as the one in Figure 2 since it has a worse fidelity score. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "In the case *global* explainability, the surrogate model $w$ approximates the black-box model $b$ over a dataset $\\mathcal{X} = \\{x_1, x_2, \\dots, x_n\\} \\subseteq \\mathcal{D}$. Distribution in $\\mathcal{X}$ should closely resemble that of $\\mathcal{D}$ for the explanation to be plausible. \n",
    "\n",
    "In local explanation, the surrogate model $w$ is an approximation to a single instance of data with a neighborhood, rather than the whole of the dataset. or $\\mathcal{X} = \\{x^{\\prime}|x \\in \\mathcal{N}(x) \\}$ and $\\mathcal{N(x)}= \\{x \\in \\mathcal{X} | d(x,x^{\\prime}) < \\epsilon\\}\\text{; where } d \\text{ is distant measure in a topology.}$ In simple case of Euclidean space: $\\mathcal{N(x)}= \\{x \\in \\mathcal{X}:\\  |x - x^{\\prime}| < \\epsilon\\}$.\n",
    "\n",
    "LIME and SHAP are two of the most commonly used local explanability models using surrogates into which we take an in-depth look as they are crucial for explaining some deep learning based model, specially for transformer-based NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929b271",
   "metadata": {},
   "source": [
    "# LIME (Local Interpretable Model-agnostic Explanation)\n",
    "LIME and SP-LIME\n",
    "Lime is a post-hoc, model agnostic local explanation using surrogate models.\n",
    "SP-Lime uses a set pf representative examples to address trusting the model problem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f4a24f",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "The idea behind Lime is to perturb the input and see how it affects predictions. Now if we perturb the data in the input that makes sense to humans, we can see how the prediction change in relation to those human-interpretable input. For instance, changing whole words and parts of the input image like w whole eye that makes sense to humans can be changed and we can observe how it changes the prediction. It is worth noticing that the model might be, and is very likely to be using, a much more complex mechanism based on much more granular data and application of purturbation results in approximation to a much simpler model. This simpler model is the explainer model. \n",
    "Additionally LIME provides local explainability, meaning that using the simpler model--e.g. linear model--LIME explains a decision in relation to a specific datapoint and its immediate neighborhood. Locality is achieved though weighted sampling based on similarity to the point that is being explained. It is obvious that it is much simpler to explain a model locally rather than on the entire model. \n",
    "Now the question is that what constitutes a human understandable concept and how a neighborhood should look like. Going back to early days of ANN, one of the issues philosophers of mind took with ANN's information representation was lack of information constituency, meaning that intermediary computational components of a neural network (weights and connections) did not have a quality Paul Smolensky labeled \"constituent structure of mental state.\" in his critique of \"connectionism\". In essence, he meant that the mental state represented in a neural network did not make any sense to the human observers. We can now see those days in the late 1980's in hour back mirror with a sense of nostalgia and a slight irritation, but the concept of constituency of latent representation as what can make a good human-understandable explanations. The authors of LIME paper use terms such as super pixels where a collection of adjacent data points represent a mental state that has constituent structure. An example of such representation is face detection. A human face needs to have certain criteria of having an approximation to a certain facial organs in a certain spatial order. Super pixels here represent those facial organs and their spatial relationships as is visualized in Figure 1. Another such example is detecting presence of a frog in an image. Authors of LIME paper have published a blog in which they use the example of a frog.\n",
    "\n",
    "<figure>\n",
    "    <img src='frog1.jpg' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 4: Transforming an image into interpretable components. Sources: Marco Tulio Ribeiro, <a href=\"https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/\">Pixabay</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Now, how do we get from an image with data that has structural constituent structure to an explanation? The idea is simple and common in almost all of the local explainability models. We create perturb the input data by turning off some of the interpretable components and get a probability score for that original model (a tree frong being in the picture). After repeating this process for a number of purturbations, we learn a linear model on the purturbed dataset. Weighting helps with local explainability as it underscores mistakes in the purtubed instances rather than global performance of the model. In this case, superpixels with highest weight constitute explanations. Figure 5 demonstrates this process:\n",
    "<figure>\n",
    "    <img src='frog2.jpg' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 5: Figure 4. Explaining a prediction with LIME: 1- The original data is transfored to the dataset with purturbations. 2- some of the regions of super pixels are turned off. 3- A regression model learns the effect of omitting turned off regions on the original prediction. 4- Highest impact regions are presented as explanations. Sources: Marco Tulio Ribeiro, Pixabay.. Sources: Marco Tulio Ribeiro, <a href=\"https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/\">Pixabay</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c18bce",
   "metadata": {},
   "source": [
    "## Fidelity-Interpretability Trade-off\n",
    "\n",
    "An explanation is a model $ g \\in G$, where $G$ is a class of potentially interpretable models. Note that $g$ act on absence/presence of an explainable model, thus domain of $g$ is interpratable data representation or $\\{0,1\\}^{d^\\prime}$.\n",
    "not every $g \\in G$ is human interpretable, this leads to introducing a measure of complexity $\\Omega(g)$. The complexity could for instance be depth of a tree or non-zero weights. The goal of LIME is to find a highest fidelity while the value of complexity is at the lowest. The LIME paper introduces $\\Pi_x(z)$ as proximity between instance $z$ and data $x$ (creating locality), where building an explanator for model $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ for classification models (could be reduced to a binary classification). \n",
    "As we saw earlier in this post, we would need to solve an optimization problem with minimizing average distance between explanation and prediction. Lime has added the complexity to the same optimization problem to create a fidelity-interpretability trade-off. More formally the paper has defined loss function $\\mathcal{L}(f,g,\\Pi_x)$ as opposite of faithfulness of explanator $g$ to model $f$. LIME then is obtained by solving the following optimization problem:\n",
    "$$\n",
    "\\xi(x) = \\text{arg} \\min\\limits_{g \\in G} \\mathcal{L}(f,g,\\Pi_x) + \\Omega(g)\n",
    "$$\n",
    "Minimizing $\\Omega$ alone reduces the complexity and thus makes the explanation more human-friendly, whilst minimizing $\\mathcal{L}$ maximizes local fidelity; therefore minimizing $\\xi$ balances between intrpretability and local fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5605d65",
   "metadata": {},
   "source": [
    "## Model Agnostic \n",
    "We saw earlier that a desired explanation should be model agnostic. We have so far, successfully translated explanation to an optimization problem on a loss function $\\mathcal{L}$ for model $g$. This is now training a model whose job is to provide a generalized solution to explainability. Like training any model, we need data and the data should result in $g$ being model agnostic after training. \n",
    "training data is sampled uniformly at random around $x^\\prime$ The data is sampled from amongst non-zero elements and is weighted by $\\Pi_x$. loss function $\\mathcal{L}$ is then approximated on the sampled data. As you remember $\\Pi_x(z)$ as proximity between instance $z$ and data $x$. Using $\\Pi$ as weight we create centers of locality in order to achieve high fidelity score using a generalized and model agnostic process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b76add",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='LIME.png' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 6: In this simple model we can see that the model $f$ that is being explained (blue background) is too complex to explain globally. Using LIME we can create a linear local explanation for $x^\\prime$ can be achieved through a linear model within a neighborhood of $x^\\prime$ (bold red cross). You can see that using the weight points (crosses) are sampled from much wider area of the dataset but the weighting mechanism enforces  locality. The dashed line is the linear model that is used for local explainability, even though the whole of the model could not be explained using a linear model.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eaf0bc",
   "metadata": {},
   "source": [
    "## Sparse linear Explanations\n",
    "As we see in the toy example in the image above, if the neighborhood is small enough, local explanations can be achieved through a linear model. The paper then introduces local explanations based on linear models, meaning that $G$ is taken to be a class of linear models, such that $g(z^\\prime) = w_g . z^\\prime$. We also saw in the toy example data sampling is a weighted distance from the local point $x^\\prime$ that is being explained. In keeping with that the LIME paper opts for weighted square loss as loss function $\\mathcal{L}$ or \n",
    "$$\n",
    "\\mathcal{L}(f,g,\\Pi_x) = \\sum_{z,z^\\prime in \\mathcal{Z}}\\Pi_x(z)(f(z)-g(z^\\prime))^2\n",
    "$$\n",
    "The equation above takes average distance of the points in vicinity of the points that is being explained and applies distance from that point as weight. The farther the point, the smaller the contribution in the calculation. Something that the size of crosses in the toy example underlines. As the weight itself is a measure of distance, we need to include a distance measure in the weight function. The weight function itself is an exponential kernel where $D$ is a distance measure. $D$ can be chosen in a way that suits the sort of model that needs to be explained. For instance it can be $cosine$ similarity in the case of text classification or L2 distance for images.\n",
    "$$\n",
    "\\Pi_x(z) = e^{ \\frac{-D^(x,z)^2}{\\sigma^2} }\n",
    "$$\n",
    "The use of kernel is basically stemmed from the fact that we are trying to find a linear approximation for a non-linear problem. \n",
    "The choice of an exponential kernel is due to the fact that exponential kernels, like other radial basis function kernels, perform well in multidimensional regression problems. In their core, such kernels look at distance between points in the dataset to a fixed point (the point that is being explained in our case) and use variance as a sensitive distance hyperparameter. \n",
    "In the imge below that represents  we can see that the value of $\\Pi$ is inversely exponentially relative to value of $z$ and thus diminishing the weight as value of $D$ (distance measure) increases. tuning $\\sigma$ (variance) is most crucial in a good kernel setup here. As you can see in the image $\\sigma$ effects behaviour of the kernel dramatically for the same data. \n",
    "\n",
    "<figure>\n",
    "    <img src='exponential.png' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 7: Figure 4. Exponential kernel. This is an exponential kernel on a 100 data points sampled as a normal distribution. We can see the effect of $\\delta$ on the shape of the graph. It is essential that we get the value of hyperparameter  $\\delta$ right to have the best possible explanation. punishing the farther points too severely and the superpixels become too small, and too low, they superpixels spill into one another, muddying the explanation. It is also worth noting that as the value increases the curvature of the kernel diminishes and it increasingly behaves as a linear kernel.</figcaption>\n",
    "</figure>\n",
    "\n",
    "For more information on kernels please check: https://www.cs.toronto.edu/~duvenaud/cookbook/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378204e0",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='exponential.png' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 7: Figure 4. Exponential kernel. This is an exponential kernel on a 100 data points sampled as a normal distribution. We can see the effect of .</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2b41b",
   "metadata": {},
   "source": [
    "## Data Sampling\n",
    "instances are being sampled around $x^\\prime$, the point that is being explained with the following characteristics.\n",
    "- They are weighted by $\\Pi_x$\n",
    "- They are non-zero, so that they positively participate in prediction.\n",
    "- They are sampled uniformly at random.\n",
    "\n",
    "Given a perturbed sample $ x^{\\prime} \\in \\{0,1\\}^{d^\\prime}$, predictions of the original model $\\{f(z):\\  z \\in \\mathbb{R}^d\\}$ are used as label for the explanation model.\n",
    "\n",
    "Then a dataset $\\mathcal{Z}$ is being built from the sampled perturbed data and their labels, which is used to optimize the LIME equation and obtain:\n",
    "$$\n",
    "\\xi(x) = \\text{arg} \\min\\limits_{g \\in G} \\mathcal{L}(f,g,\\Pi_x) + \\Omega(g)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c6196d",
   "metadata": {},
   "source": [
    "# Example: Object detection explanation\n",
    "In this example we try using LIME's image explainer on 4 pictures, two of lions and two of cheetahs using two different deep learning models, inception and resnet152. We then look at the regions that are picked for detecting the animal and see if they make sense to humans. We start with the following high resolution images. We then transforms the images and run them through our prediction models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543f8dd",
   "metadata": {},
   "source": [
    "## Original Images\n",
    "<figure>\n",
    "    <img src=\"data/originals.jpg\" alt='Original Images'  style=\"float:left\"/>\n",
    "    <figcaption>Figure 8: Original images. Next we 1) transform the images, 2) make predictions using two different algorithms, 3) produce explanations, and 4) analyze the explanations.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f3ce0",
   "metadata": {},
   "source": [
    "## Images after transformation\n",
    "\n",
    "```python\n",
    "# We center crop the image with a large enough space for the main subject to be included in the picture.\n",
    "# I tried 1500 pixels and it worked for all three pictures. Then using transform, we resuze the picture to \n",
    "# the size that is compatible with ImageNet based algorithms. \n",
    "def get_input_transform():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])       \n",
    "    transf = transforms.Compose([\n",
    "        transforms.CenterCrop(1500),\n",
    "        transforms.Resize((256, 256)),        \n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])    \n",
    "\n",
    "    return transf\n",
    "\n",
    "def get_input_tensors(img):\n",
    "    transf = get_input_transform()\n",
    "    return transf(img).unsqueeze(0)\n",
    "```\n",
    "\n",
    "## Applying the transformation\n",
    "```python\n",
    "transformed_images = []\n",
    "nimgT = transforms.ToPILImage()\n",
    "for image in img:\n",
    "    get_input_tensors(image)[1:].shape\n",
    "    transformed_images.append(nimgT(get_input_tensors(image).squeeze()))\n",
    "```\n",
    "## transformed images \n",
    "<figure>\n",
    "    <img src=\"data/processed.png\" alt='transformed images'  style=\"float:left\"/>\n",
    "    <figcaption>Figure 9: As it can be observed, the processed images are already not useful to the humans. Next we 1) make predictions using two different algorithms, 2) produce explanations, and 3) analyze the explanations.</figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a8740",
   "metadata": {},
   "source": [
    "## Choosing models we want to explain\n",
    "```python\n",
    "model1 = models.inception_v3(pretrained=True)\n",
    "model2 = models.resnet152(pretrained=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17264f",
   "metadata": {},
   "source": [
    "## Model Predictions\n",
    "After loading the models we run a prediction on both models. The results are summarized in table 1:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"data/inception.png\" alt='predictions: inception v3'  style=\"float:left\"/>\n",
    "    <figcaption></figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=\"data/resnet152.png\" alt='predictions: resnet152'  style=\"float:left\"/>\n",
    "    <figcaption>Table1:predictions. The table on the top includes the top5 predictions from inception v3 and the table below that includes the results from resnet-152. We can see that inception is quite certain about its top choice. Still we do not know why and how the model has made such decision or how did animals like gazelle made it to the top5, however low the priority maybe. the same story applies to resnet, except that for teh first image the prediction gives a 70% change that the image depicts cheetahs.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c25b1eb",
   "metadata": {},
   "source": [
    "## Preparing explanations\n",
    "We now Prepare to create an explainer. Let us remember what the process for creating location Lime creates an array of images through perturbation algorithm from the original image, It then uses the perturbed data to train an explainer model. This means we need to provide the original image and the original classification function so that LIME can produce probabilities for the perturbed dataset. \n",
    "In the next step we prepare or provide the following to LIME to train a generic explainer using: `lime.lime_image.LimeImageExplainer()`.\n",
    "\n",
    "1. Classifier $f$ that we aim to explained. In our case inception-v3 and resnet-152 respectively.\n",
    "2. pre-processed original images $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ee66e",
   "metadata": {},
   "source": [
    "### Data Transformers\n",
    "```python \n",
    "def get_pil_transform(): \n",
    "    transf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224)\n",
    "    ])    \n",
    "\n",
    "    return transf\n",
    "\n",
    "def get_preprocess_transform():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])     \n",
    "    transf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])    \n",
    "\n",
    "    return transf    \n",
    "\n",
    "pill_transf = get_pil_transform()\n",
    "preprocess_transform = get_preprocess_transform()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77309ce",
   "metadata": {},
   "source": [
    "`pill_transf` is a transformer based on `get_pil_transform` that takes an image and performs standard resizing and center cropping for imagenet-based models. Obviously you can change the transformer based on whatever model use are using as high-resolution models have gained popularity in recent years.\n",
    "\n",
    "*`pill_transf` is used by the original model $f$ to make predictions on the original image.*\n",
    "\n",
    "`preprocess_transform` is another transformer that basically normalizes the image based on a pre-defined mean and variance per dimension. \n",
    "\n",
    "*`preprocess_transform` is used to generate an instance explainer where the perturbation process is applied and the explainer model is trained.*\n",
    "\n",
    "Given an image `pill_transf` returns tensors of the shape (3, 224, 224), which is in channel first layout or BCHW (Batch, Channel, Height, Width and as we are processing only a single image $x$, B=0 and is omitted). If you are using frameworks that require channel last or BHWC, then you need to transpose the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625154d9",
   "metadata": {},
   "source": [
    "### Predictor\n",
    "As we have seen we are using inception-v3 and resnet-152, respectively denoted by model1 and model2 in this example.\n",
    "\n",
    "The following function `batch_predictor`, simply uses the model to make a prediction. The predictor function is later passed to `lime.lime_image.LimeImageExplainer()` in order to make predictions on the perturbed data.\n",
    "\n",
    "```python\n",
    "def batch_predict1(images):\n",
    "    model1.eval()\n",
    "    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model1.to(device)\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    logits = model1(batch)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs.detach().cpu().numpy()\n",
    "```\n",
    "This function simply take a list of images and perform prediction using the original model. This specific function uses our `model`, which we defined earlier as `model1 = models.inception_v3(pretrained=True)`\n",
    ". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5dfbee",
   "metadata": {},
   "source": [
    "### Generating the local explainer\n",
    "`explainer` is defined as a `LimeImageExplainer`. As seen in the LIME paper, for categorical features, `LimeImageExplainer` perturbs the data by sampling according to the training distribution, and making a binary\n",
    "feature that is 1 when the value is the same as the instance being explained. Kernel width can be passed to this function as a parameter.  \n",
    "\n",
    "As we are performing a local explanation, we call `explain_instance` method of our `explainer` and pass the original image passed through `pill_transf` along with other parameters amongst them most notably is `num_samples`, which as the name indicates, defines the number of perturbed samples. \n",
    "\n",
    "`explain_instance` returns a `lime.lime_image.ImageExplanation` object that contains data about explanations. Most notably it includes segmentation information `ImageExplanation.segment` and make information `ImageExplanation.get_image_and_mask()`\n",
    "\n",
    "```python\n",
    "from lime import lime_image\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanations1 = []\n",
    "explanations2 = []\n",
    "for image in img:\n",
    "    explanations1.append(explainer.explain_instance(np.array(pill_transf(image)), \n",
    "                                         batch_predict1, \n",
    "                                         top_labels=5, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000) )\n",
    "    explanations2.append(explainer.explain_instance(np.array(pill_transf(image)), \n",
    "                                         batch_predict2, # classification function\n",
    "                                         top_labels=5, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000) )\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "At the next step we use `ImageExplanation.segment` to visualize segmentation information as a way of explaining which regions of image were used by the predictor model to determine the classification probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e00693",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Finally we use skimage to visualize the segmentation we have acquired through `ImageExplanation.get_image_and_mask().`\n",
    "\n",
    "```python\n",
    "from skimage.segmentation import mark_boundaries\n",
    "ib = []\n",
    "for explanation in explanations1:\n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
    "    ib.append(mark_boundaries(temp/255.0, mask))\n",
    "fig = plt.figure(figsize=(15., 15.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(1, 3),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, ib):\n",
    "    ax.imshow(im)\n",
    "```\n",
    "\n",
    "This is only the code sample for model1-e.ginception-v3. You will find in the accompanying code notebook, that we have performed the computation for both models. Next we can see the result of visualization for all three images and both models. It is notable that the segmentation includes the environment and not just the object itself. This can explain why completely different looking savanna animals were included in the top5 selection. This, however is a hypothesis as trying the model with other animals such as gazelle could clarify the role of background, environment, and climate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eefea1b",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"data/inceptionexplain.png\" alt='predictions: inception v3'  style=\"float:left\"/>\n",
    "    <figcaption>Figure 10: Segmentation generated by LIME. The regions in this segmentation are used to determine the classification of the object in the image. The underlying model in this case is inceptino-v3 and LIME has performed as a model agnostic post-hoc local explainer. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472824fb",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"data/resnetexplain.png\" alt='predictions: resnet-512'  style=\"float:left\"/>\n",
    "    <figcaption>Figure 11: Segmentation generated by LIME. The regions in this segmentation are used to determine the classification of the object in the image. The underlying model in this case is resnet-512 and LIME has performed as a model agnostic post-hoc local explainer. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe69f3",
   "metadata": {},
   "source": [
    "# References\n",
    "- lime: chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1602.04938v1.pdf\n",
    "- survey: chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2011.07876.pdf\n",
    "- https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/\n",
    "- https://github.com/marcotcr/lime/tree/master/doc/notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3209c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
