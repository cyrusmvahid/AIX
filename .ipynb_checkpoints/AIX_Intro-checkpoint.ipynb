{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95496219",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to AI Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecff2d6",
   "metadata": {},
   "source": [
    "## Context\n",
    "This blog series focuses on model explainability for supervised learning both for blackbox models such as deep learning models as well as interpretable models such as decision trees. This blog explains motivations for explainability as well as laying out basic definitions. The subsequent set of publications, dubbed *intermediate*, will describe basic explainability methods such as SHAPLY and associated software libraries. The final segment of the present publication ill focus on implementation of model explainability in blackbox domain specific fields such as transformer based architecture for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb076e",
   "metadata": {},
   "source": [
    "## What is explainability\n",
    "In its core, model explainability is to make decision process of a model, understandable for humans, regardless of the data representation or its intermediary encodings. For instance for a language model an explanation could be presence of a collection of words with certain proximity to one another. This is for instance a bag of words solution, but we know that attention-based modern neural models are much more accurate than statistical models based on bag of words. An explanation for a transformer -based model can be a bag of word approximation on the inference. We therefore can enjoy accuracy of more complex but opaque model, while explain its behaviour consistently with a simpler and less accurate model. Same logic can be applied on time series models making trading decisions or computer vision models. For instance a human interpretable data representation for computer vision, could be a *super-pixel*, a collection of pixels always appearing together that are recognizable by humans, rather than single pixels that a computer vision model might be focusing on. this would be the case with masks in case of convolutional neural networks that when visualized could represent a schematic image of an eye or a nose.      \n",
    "<figure>\n",
    "    <img src='cnn.jpeg' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 1: visual representation of a CNN model for face detection</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407fca8",
   "metadata": {},
   "source": [
    "## Reasons for explainability\n",
    "[Burkart, Hurber] identify trust, causality, transferability, informativeness, fairness, accountability, making adjustments, and proxy functionality as the most common reasons for the need for AIX. Let us explore some of these reasons in more details. \n",
    "### Trust\n",
    "One of the major issues of automation, specially the kind of automation that is achieved through probabilistic models, is trust of humans. For instance, we trust a human driver is context aware and makes the right decision when needed most of the time, while many people do have doubts about ability of self-driving cars to make the correct decision at the correct time all the time. The ability to interpret a model helps building both emotional and legal trust in automated decision making. There are two types of trust that someone who is affected by an inference needs to have on an AI-based system: trust in a specific prediction, which culminates in global explainability and trust the model as a whole will make the right decisions. This sort of trust is achieved through local explainability. Both these concepts are explained later in this blog.\n",
    "### Causality\n",
    "Most ML inference work out probabilistic correlation of two or more phenomena and have no insight into causal structure of events, saving causal inference [[more information at CleaR conference]] (https://www.cclear.cc/AcceptedPapers). For instance, the causal relation between emergence of birds of spring, such as swift, and arrival of the season has been clear for as long as human have lived in those regions where migrating birds arrive just before spring. People do acknowledge that there is a correlation between arrival of the migrant birds and changing of the season, but I do doubt anyone would consider migrant birds are causing spring to arrive.\n",
    "### Fairness\n",
    "You might want to know why you are denied a loan application or a bank account. Opaque decision making does not help establish fairness. There has been numerous studies that human bias has resulted rejection of applicants whose name indicate certain ethnicity or religion. The same has been shown to be occuring based on postal codes and other demarcation factors. In many the disparity in providing opportunity is not intentional, but rooted in human bias. The algorithms are not immune to the same biases. The ability to explain and interpret how a decision is made, can help improve both data and algorithms to make fairer and more ethical decisions.     \n",
    "\n",
    "### accountability\n",
    "There are legal frameworks in place that require humans to justify their decisions. In Germany, for instance, an employer is obliged to provide a rejected candidate with reasons for rejection should the candidate requests such justification. Energy companies are required to provide justification for trading decisions if they own both production and distribution of energy in the supply chain. Car accidents require investigation so that intent and  blame can be assigned to those involved. There are countless other examples that humans are required to justify their decisions. As algorithms are increasingly becoming autonomous decision makers, their decisions also need to be able to justified in accordance with existing and emerging legal requirements.\n",
    "\n",
    "In addition to criteria for improving quality of decision making by the machine, we can help humans learn from better algorithms. If an algorithm becomes consistently better at humans at making certain decisions, which is the goal of automation through AI, studying the decision process can help humans learn from the algorithm and improve their decision process and thus creating a mutually reinforcing decision improvement feedback loop between man and machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31f413",
   "metadata": {},
   "source": [
    "## Supervised learning: definitions\n",
    "### Model \n",
    "model $h(x)=y$ is a supervised learning model where $x\\in \\mathcal{X} \\subseteq \\mathbb{R}^{d \\times l}$ and $y \\in \\mathcal{Y} \\subseteq \\mathbb{R}^k$. In the case of SML a set of labeled data $\\mathcal {D_{\\tau}} = \\{(x_1, y_1)\\ , \\dots ,(x_n, y_n)\\}$ is used to train the model in order for the model to to be able to map $h$ over unseen data $\\mathcal{X^\\pi}=\\{x^1, \\dots, x^k\\}$ to prediction $\\mathcal{Y}=\\{y^1, \\dots, y^n\\}$. In the simple case of single class classification and regression $l=k=1$.\n",
    "### Blackbox model\n",
    "A blackbox model in our context $b: \\mathcal{X} \\rightarrow \\mathcal{Y}$, $b\\in \\mathcal{B}$, where $\\mathcal{B}$ is the hypothesis space for a deep learning model.\n",
    "### Error\n",
    "To evaluate a model we use an error measure that uses some topological distance mechanism on the output manifold to measure distance of a prediction to an observed value or $\\mathcal{E} = p-o$, where $p$ is a predicted value and $o$ is an observed value. For instance RMSE (Root Mean Square Error) is a type of Euclidean distance that uses the euclidean distance between two points in a multi-dimensional space. \n",
    "$$RMSE = \\sqrt{{\\frac{1}{n}} \\sum_{i=1}^n(y_i-x_i)^2 }$$.  \n",
    "### Learning\n",
    "Given dataset $\\mathcal{D}$, SML attempts to solve optimization problem: \n",
    "$$\n",
    "\\large{ h^*=\\text{arg} \\min\\limits_{h \\in \\mathcal{H}}\\mathcal{E}(h(x)) }\n",
    "$$. \n",
    "\n",
    "In the case of parameteric models such as a deep learning model where the model parameters are represented as $\\theta$ and optimized parameters as $\\theta^*$, the optimization problem can be formulated as: \n",
    "\n",
    "$$\n",
    "\\large{ \\theta^*=\\text{arg} \\min\\limits_{\\theta}\\mathcal{E}(h(x; \\theta)) }\n",
    "$$.\n",
    "\n",
    "For most models solution to the optimization problem is often not unique. The matter is exasperated for complex models, specially in deep learning domain where we are faced with non-convex optimization and $\\theta^*$ is only an approximation to a acceptable local minima. Other issues such as dataset biases make the matter of interpretability even more uncertain. We, therefore, need to find a solution as to explain why a model has made a decision based on a set of circumstances.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032ccfe",
   "metadata": {},
   "source": [
    "## Explainability Approaches\n",
    "In the context of this blog, there are in general two kinds of explainability, local and global. Local explainability, aka instance explainability, deals with explaining how a decision was made for a single input data $x \\in \\mathcal{D}$. This means that the explanation is valid only for $x$ and its close vicinity. Global, or model, explanation approach, generate explanations for how a model in general arrives at a decision on a dataset $\\mathcal{D}$. \n",
    "**Example**: An face detection system has recognized a person to be a person of interest. Local explainability should be clarify as why that person was matched against a known face. Global explainability, should be able to lay out how the face detection works. One of simplest ways is the projection of internal state of nodes in an image recognition model.\n",
    "\n",
    "*The task of model explanation aims to generate a human understandable interpretation **explanator** for the learning process by extending or modifying the learning process described in the previous section*\n",
    "\n",
    "Please bear in mind that explanators differ from predictors as the former always rely on the latter to perform the explanation task.\n",
    "\n",
    "Table 1 provided a comprehensive classification of explainability approaches.\n",
    "\n",
    "| ExplainabilityApproach | Description                                                                  |\n",
    "|:---                     |:---                                                                            |\n",
    "| ante-hoc               | Explainability is built into the model.                                      |\n",
    "| pos-hoc                | Explainability is created after model creation.                                                            |\n",
    "| instance/local         | Explainability is is only applicable to a single instance of data and its close vicinity.                  |\n",
    "| agnostic               | Explainability is independnt of the model itself and is applicable to many or all models.                  |\n",
    "| data independent       | Explainability mechanism works without additional data and is applicable to many or all relevant datasets. |\n",
    "| data dependent         | Explainability requires data.                                                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c725246",
   "metadata": {},
   "source": [
    "## Model Explanators\n",
    " model explanator takes a model $\\mathcal{x}[input] \\implies \\mathcal{Y}[predictions]$ and a specific labeled dataset as an input and creates and explanation belonging to the set $\\mathcal{E}[explanations]$. More formally:\n",
    " $$\n",
    " \\large{ e: \\rightarrow (\\mathcal{X} \\rightarrow \\mathcal{Y}) \\times (\\mathcal{X}\\times \\mathcal{Y}) \\rightarrow \\mathcal{E} }\n",
    " $$\n",
    " As mentioned in the previous sections, there are two approached to explanation: global and local. In the case of global explanations, explanator $e$ take a model $b$ and a dataset $\\mathcal{D'}$ or $e(b, \\mathcal{D'}$. Local implementation, takes a model $b$ and an instance from the dataset $(x,y) \\in \\mathcal{D'}$ as input. This is consistent with the intuitive definition that global explanators explain a model's inferences over a specific dataset and local explantors provide interpretation for a specific instance of data belonging to a dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f8c3c",
   "metadata": {},
   "source": [
    "## Characteristics of Model Explanators\n",
    "**Interpretability** Interpretability in essence is quantification of understanding of how input values affect one another. For instance how income and the number of dependent children can affect approval score for a loan. It is essential to factor in human limitations as the goal of interpretability to make decisions of a model understandable to humans. An explanation with hundreds of dimensions is basically useless of an interpretation for humans. reducing dimensionality to top 5 or 10 important factors that most significantly affect decisions made by the model is the valid approach. \n",
    "\n",
    "**Local Fidelity:** A surrogate model is an approximation to the original model. The surrogate models tend to be less accurate and indeed not factoring in all the parameters used by the inference model. The explanator still needs to be locally faithful at the instance that is being explained and at its immediate neighborhood. Importance of local fidelity is that local decisions might be influenced by a different set of parameters that differ from a decision to another. Focusing only on global parameters might not be able to explain individual decisions, at least based on small enough set of parameters that are human interpretable.\n",
    "\n",
    "**Model Agnostic:**\n",
    "    An explainer should be able to be trained to explain any model given a dataset and black-box model. This is the same criteria that is required for models themselves to be good at generalization. \n",
    "    \n",
    "**Global Prespective:** Apart from trusting a specific local decision, it is also important to trust the model itself to be making the right decisions most of the time. This is similar to trusting decision making of a human, even though on occasions the human might make sub-optimal decisions. Also making good decisions on occasions, does not make a human a generally good decision maker. So for an explainer to be trusted it has to make good local decisions most of the time; thus incorporating local fidelity and global perspective.s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb341e4",
   "metadata": {},
   "source": [
    "## Post-hoc model explanation for black-box modeles via surrogate models\n",
    "As the name suggests, a black box model is a model, whose innerworkings are opaque to human understanding, such as deep neural networks. This is not a new sort of recognized problems. Even in the early days of neural networks criticism was levied at opaqueness of logic of neural networks through information constituency argument. One way to explain black-box models' inference is to develop a surrogate model that is a white box model capable of explaining decisions that the black-box models make. This way we can benefit from internal complexity of a black-box model such as a ANN, while use a simple model that consistently explains the inferences the more complex model has created. Do bear in mind that justification for effectiveness of neural network is based on *universal approximation theorem*, which in turn proposes to solve complex non-linear problems using approximation to a function that behaves similarly to the problem that we want to solve. \n",
    "\n",
    "Figure 2 shows a simplified version of a simple approximation (blue graph) that can interpret the more complicated inference graph (orange one). This is foundation of machine learning in general. If the distance between predictions and observed is within a certain range we can justify the models' prediction posthoc.\n",
    "\n",
    "<figure>\n",
    "    <img src='approximation.png' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 2: Simple approximation. The complicated orange graph can be approximated and explained by the simpler blue graph </figcaption>\n",
    "</figure>\n",
    "\n",
    "More formally, finding a surrogate model results in solving the problem that is formulated below. In simpler terms, explainability though surrogate models is the process of fitting an explainable model $w$ to make predictions where the average distance between the the outcome of the surrogate model and predictions of the black-box model is bounded:\n",
    "$$\n",
    "\\large{ w^* = \\text{arg} \\min\\limits_{x \\in \\mathcal{\\chi}} \\frac{1}{|\\mathcal{X}|} \\sum_{x \\in \\mathcal{X}}S(w(x), b(x)) }\n",
    "$$\n",
    "$S$ is called the ***fidelity score*** and is a measure of how well the white-box surrogate model $w$ approximates the black-box model $b$. The smaller the value for $S$ is, the better the approximate is.\n",
    "\n",
    "We can see that the in figure 2, the fidelity score is twice as high as the one in Figure 1, and hence the approximation is less accurate. \n",
    "\n",
    "<figure>\n",
    "    <img src='approximation1.png' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 3: Even though the orange graph still represents an approximation of the blue graph, it is not as good as the one in Figure 2 since it has a worse fidelity score. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "In the case *global* explainability, the surrogate model $w$ approximates the black-box model $b$ over a dataset $\\mathcal{X} = \\{x_1, x_2, \\dots, x_n\\} \\subseteq \\mathcal{D}$. Distribution in $\\mathcal{X}$ should closely resemble that of $\\mathcal{D}$ for the explanation to be plausible. \n",
    "\n",
    "In local explanation, the surrogate model $w$ is an approximation to a single instance of data with a neighborhood, rather than the whole of the dataset. or $\\mathcal{X} = \\{x^{\\prime}|x \\in \\mathcal{N}(x) \\}$ and $\\mathcal{N(x)}= \\{x \\in \\mathcal{X} | d(x,x^{\\prime}) < \\epsilon\\}\\text{; where } d \\text{ is distant measure in a topology.}$ In simple case of Euclidean space: $\\mathcal{N(x)}= \\{x \\in \\mathcal{X}:\\  |x - x^{\\prime}| < \\epsilon\\}$.\n",
    "\n",
    "LIME and SHAP are two of the most commonly used local explanability models using surrogates into which we take an in-depth look as they are crucial for explaining some deep learning based model, specially for transformer-based NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929b271",
   "metadata": {},
   "source": [
    "# LIME (Local Interpretable Model-agnostic Explanation)\n",
    "LIME and SP-LIME\n",
    "Lime is a post-hoc, model agnostic local explanation using surrogate models.\n",
    "SP-Lime uses a set pf representative examples to address trusting the model problem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f4a24f",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "The idea behind Lime is to perturb the input and see how it affects predictions. Now if we perturb the data in the input that makes sense to humans, we can see how the prediction change in relation to those human-interpretable input. For instance, changing whole words and parts of the input image like w whole eye that makes sense to humans can be changed and we can observe how it changes the prediction. It is worth noticing that the model might be, and is very likely to be using, a much more complex mechanism based on much more granular data and application of purturbation results in approximation to a much simpler model. This simpler model is the explainer model. \n",
    "Additionally LIME provides local explainability, meaning that using the simpler model--e.g. linear model--LIME explains a decision in relation to a specific datapoint and its immediate neighborhood. Locality is achieved though weighted sampling based on similarity to the point that is being explained. It is obvious that it is much simpler to explain a model locally rather than on the entire model. \n",
    "Now the question is that what constitutes a human understandable concept and how a neighborhood should look like. Going back to early days of ANN, one of the issues philosophers of mind took with ANN's information representation was lack of information constituency, meaning that intermediary computational components of a neural network (weights and connections) did not have a quality Paul Smolensky labeled \"constituent structure of mental state.\" in his critique of \"connectionism\". In essence, he meant that the mental state represented in a neural network did not make any sense to the human observers. We can now see those days in the late 1980's in hour back mirror with a sense of nostalgia and a slight irritation, but the concept of constituency of latent representation as what can make a good human-understandable explanations. The authors of LIME paper use terms such as super pixels where a collection of adjacent data points represent a mental state that has constituent structure. An example of such representation is face detection. A human face needs to have certain criteria of having an approximation to a certain facial organs in a certain spatial order. Super pixels here represent those facial organs and their spatial relationships as is visualized in Figure 1. Another such example is detecting presence of a frog in an image. Authors of LIME paper have published a blog in which they use the example of a frog.\n",
    "\n",
    "<figure>\n",
    "    <img src='frog1.jpg' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 4: Transforming an image into interpretable components. Sources: Marco Tulio Ribeiro, <a href=\"https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/\">Pixabay</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Now, how do we get from an image with data that has structural constituent structure to an explanation? The idea is simple and common in almost all of the local explainability models. We create perturb the input data by turning off some of the interpretable components and get a probability score for that original model (a tree frong being in the picture). After repeating this process for a number of purturbations, we learn a linear model on the purturbed dataset. Weighting helps with local explainability as it underscores mistakes in the purtubed instances rather than global performance of the model. In this case, superpixels with highest weight constitute explanations. Figure 5 demonstrates this process:\n",
    "<figure>\n",
    "    <img src='frog2.jpg' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 5: Figure 4. Explaining a prediction with LIME: 1- The original data is transfored to the dataset with purturbations. 2- some of the regions of super pixels are turned off. 3- A regression model learns the effect of omitting turned off regions on the original prediction. 4- Highest impact regions are presented as explanations. Sources: Marco Tulio Ribeiro, Pixabay.. Sources: Marco Tulio Ribeiro, <a href=\"https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/\">Pixabay</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c18bce",
   "metadata": {},
   "source": [
    "## Fidelity-Interpretability Trade-off\n",
    "\n",
    "An explanation is a model $ g \\in G$, where $G$ is a class of potentially interpretable models. Note that $g$ act on absence/presence of an explainable model, thus domain of $g$ is interpratable data representation or $\\{0,1\\}^{d^\\prime}$.\n",
    "not every $g \\in G$ is human interpretable, this leads to introducing a measure of complexity $\\Omega(g)$. The complexity could for instance be depth of a tree or non-zero weights. The goal of LIME is to find a highest fidelity while the value of complexity is at the lowest. The LIME paper introduces $\\Pi_x(z)$ as proximity between instance $z$ and data $x$ (creating locality), where building an explanator for model $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ for classification models (could be reduced to a binary classification). \n",
    "As we saw earlier in this post, we would need to solve an optimization problem with minimizing average distance between explanation and prediction. Lime has added the complexity to the same optimization problem to create a fidelity-interpretability trade-off. More formally the paper has defined loss function $\\mathcal{L}(f,g,\\Pi_x)$ as opposite of faithfulness of explanator $g$ to model $f$. LIME then is obtained by solving the following optimization problem:\n",
    "$$\n",
    "\\large{\\xi(x) = \\text{arg} \\min\\limits_{g \\in G} \\mathcal{L}(f,g,\\Pi_x) + \\Omega(g)}\n",
    "$$\n",
    "Minimizing $\\Omega$ alone reduces the complexity and thus makes the explanation more human-friendly, whilst minimizing $\\mathcal{L}$ maximizes local fidelity; therefore minimizing $\\xi$ balances between intrpretability and local fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5605d65",
   "metadata": {},
   "source": [
    "## Model Agnostic \n",
    "We saw earlier that a desired explanation should be model agnostic. We have so far, successfully translated explanation to an optimization problem on a loss function $\\mathcal{L}$ for model $g$. This is now training a model whose job is to provide a generalized solution to explainability. Like training any model, we need data and the data should result in $g$ being model agnostic after training. \n",
    "training data is sampled uniformly at random around $x^\\prime$ The data is sampled from amongst non-zero elements and is weighted by $\\Pi_x$. loss function $\\mathcal{L}$ is then approximated on the sampled data. As you remember $\\Pi_x(z)$ as proximity between instance $z$ and data $x$. Using $\\Pi$ as weight we create centers of locality in order to achieve high fidelity score using a generalized and model agnostic process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b76add",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='LIME.png' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 6: In this simple model we can see that the model $f$ that is being explained (blue background) is too complex to explain globally. Using LIME we can create a linear local explanation for $x^\\prime$ can be achieved through a linear model within a neighborhood of $x^\\prime$ (bold red cross). You can see that using the weight points (crosses) are sampled from much wider area of the dataset but the weighting mechanism enforces  locality. The dashed line is the linear model that is used for local explainability, even though the whole of the model could not be explained using a linear model.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eaf0bc",
   "metadata": {},
   "source": [
    "## Sparse linear Explanations\n",
    "As we see in the toy example in the image above, if the neighborhood is small enough, local explanations can be achieved through a linear model. The paper then introduces local explanations based on linear models, meaning that $G$ is taken to be a class of linear models, such that $g(z^\\prime) = w_g . z^\\prime$. We also saw in the toy example data sampling is a weighted distance from the local point $x^\\prime$ that is being explained. In keeping with that the LIME paper opts for weighted square loss as loss function $\\mathcal{L}$ or \n",
    "$$\n",
    "\\large{ \\mathcal{L}(f,g,\\Pi_x) = \\sum_{z,z^\\prime in \\mathcal{Z}}\\Pi_x(z)(f(z)-g(z^\\prime))^2 }\n",
    "$$\n",
    "The equation above takes average distance of the points in vicinity of the points that is being explained and applies distance from that point as weight. The farther the point, the smaller the contribution in the calculation. Something that the size of crosses in the toy example underlines. As the weight itself is a measure of distance, we need to include a distance measure in the weight function. The weight function itself is an exponential kernel where $D$ is a distance measure. $D$ can be chosen in a way that suits the sort of model that needs to be explained. For instance it can be $cosine$ similarity in the case of text classification or L2 distance for images.\n",
    "$$\n",
    "\\large{ \\Pi_x(z) = e^{ \\frac{-D^(x,z)^2}{\\sigma^2} } }\n",
    "$$\n",
    "The use of kernel is basically stemmed from the fact that we are trying to find a linear approximation for a non-linear problem. \n",
    "The choice of an exponential kernel is due to the fact that exponential kernels, like other radial basis function kernels, perform well in multidimensional regression problems. In their core, such kernels look at distance between points in the dataset to a fixed point (the point that is being explained in our case) and use variance as a sensitive distance hyperparameter. \n",
    "In the imge below that represents  we can see that the value of $\\Pi$ is inversely exponentially relative to value of $z$ and thus diminishing the weight as value of $D$ (distance measure) increases. tuning $\\sigma$ (variance) is most crucial in a good kernel setup here. As you can see in the image $\\sigma$ effects behaviour of the kernel dramatically for the same data. \n",
    "\n",
    "<figure>\n",
    "    <img src='exponential.png' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 7: Figure 4. Exponential kernel. This is an exponential kernel on a 100 data points sampled as a normal distribution. We can see the effect of $\\delta$ on the shape of the graph. It is essential that we get the value of hyperparameter  $\\delta$ right to have the best possible explanation. punishing the farther points too severely and the superpixels become too small, and too low, they superpixels spill into one another, muddying the explanation. It is also worth noting that as the value increases the curvature of the kernel diminishes and it increasingly behaves as a linear kernel.</figcaption>\n",
    "</figure>\n",
    "\n",
    "For more information on kernels please check: https://www.cs.toronto.edu/~duvenaud/cookbook/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378204e0",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='exponential.png' alt='LIME intuition' heigth=50% width=50% style=\"float:left\"/>\n",
    "    <figcaption>Figure 7: Figure 4. Exponential kernel. This is an exponential kernel on a 100 data points sampled as a normal distribution. We can see the effect of .</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2b41b",
   "metadata": {},
   "source": [
    "## Data Sampling\n",
    "instances are being sampled around $x^\\prime$, the point that is being explained with the following characteristics.\n",
    "- They are weighted by $\\Pi_x$\n",
    "- They are non-zero, so that they positively participate in prediction.\n",
    "- They are sampled uniformly at random.\n",
    "\n",
    "Given a perturbed sample $ x^{\\prime} \\in \\{0,1\\}^{d^\\prime}$, predictions of the original model $\\{f(z):\\  z \\in \\mathbb{R}^d\\}$ are used as label for the explanation model.\n",
    "\n",
    "Then a dataset $\\mathcal{Z}$ is being built from the sampled perturbed data and their labels, which is used to optimize the LIME equation and obtain:\n",
    "$$\n",
    "\\large{ \\xi(x) = \\text{arg} \\min\\limits_{g \\in G} \\mathcal{L}(f,g,\\Pi_x) + \\Omega(g) }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c6196d",
   "metadata": {},
   "source": [
    "# Example: Object detection explanation\n",
    "In this example we try using LIME's image explainer on 4 pictures, two of lions and two of cheetahs using two different deep learning models, inception and resnet152. We then look at the regions that are picked for detecting the animal and see if they make sense to humans. We start with the following high resolution images. We then transforms the images and run them through our prediction models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543f8dd",
   "metadata": {},
   "source": [
    "## Original Images\n",
    "<figure>\n",
    "    <img src=\"data/originals.jpg\" alt='Original Images'  style=\"float:left\"/>\n",
    "    <figcaption>Figure 8: Original images. Next we 1) transform the images, 2) make predictions using two different algorithms, 3) produce explanations, and 4) analyze the explanations.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f3ce0",
   "metadata": {},
   "source": [
    "## Images after transformation\n",
    "\n",
    "```python\n",
    "# We center crop the image with a large enough space for the main subject to be included in the picture.\n",
    "# I tried 1500 pixels and it worked for all three pictures. Then using transform, we resuze the picture to \n",
    "# the size that is compatible with ImageNet based algorithms. \n",
    "def get_input_transform():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])       \n",
    "    transf = transforms.Compose([\n",
    "        transforms.CenterCrop(1500),\n",
    "        transforms.Resize((256, 256)),        \n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])    \n",
    "\n",
    "    return transf\n",
    "\n",
    "def get_input_tensors(img):\n",
    "    transf = get_input_transform()\n",
    "    return transf(img).unsqueeze(0)\n",
    "```\n",
    "\n",
    "## Applying the transformation\n",
    "```python\n",
    "transformed_images = []\n",
    "nimgT = transforms.ToPILImage()\n",
    "for image in img:\n",
    "    get_input_tensors(image)[1:].shape\n",
    "    transformed_images.append(nimgT(get_input_tensors(image).squeeze()))\n",
    "```\n",
    "## transformed images \n",
    "<figure>\n",
    "    <img src=\"data/processed.png\" alt='transformed images'  style=\"float:left\"/>\n",
    "    <figcaption>Figure 9: As it can be observed, the processed images are already not useful to the humans. Next we 1) make predictions using two different algorithms, 2) produce explanations, and 3) analyze the explanations.</figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a8740",
   "metadata": {},
   "source": [
    "## Choosing models we want to explain\n",
    "```python\n",
    "model1 = models.inception_v3(pretrained=True)\n",
    "model2 = models.resnet152(pretrained=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0d0dcf",
   "metadata": {},
   "source": [
    "## Model Predictions\n",
    "After loading the models we run a prediction on both models. The results are summarized in table 1:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"data/inception.png\" alt='predictions: inception v3'  style=\"float:left\"/>\n",
    "    <figcaption></figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=\"data/resnet152.png\" alt='predictions: resnet152'  style=\"float:left\"/>\n",
    "    <figcaption>Table1:predictions. The table on the top includes the top5 predictions from inception v3 and the table below that includes the results from resnet-152. We can see that inception is quite certain about its top choice. Still we do not know why and how the model has made such decision or how did animals like gazelle made it to the top5, however low the priority maybe. the same story applies to resnet, except that for teh first image the prediction gives a 70% change that the image depicts cheetahs.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714177e5",
   "metadata": {},
   "source": [
    "## Preparing explanations\n",
    "We now Prepare to create an explainer. Let us remember what the process for creating location Lime creates an array of images through perturbation algorithm from the original image, It then uses the perturbed data to train an explainer model. This means we need to provide the original image and the original classification function so that LIME can produce probabilities for the perturbed dataset. \n",
    "In the next step we prepare or provide the following to LIME to train a generic explainer using: `lime.lime_image.LimeImageExplainer()`.\n",
    "\n",
    "1. Classifier $f$ that we aim to explained. In our case inception-v3 and resnet-152 respectively.\n",
    "2. pre-processed original images $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba836e1e",
   "metadata": {},
   "source": [
    "### Data Transformers\n",
    "```python \n",
    "def get_pil_transform(): \n",
    "    transf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224)\n",
    "    ])    \n",
    "\n",
    "    return transf\n",
    "\n",
    "def get_preprocess_transform():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])     \n",
    "    transf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])    \n",
    "\n",
    "    return transf    \n",
    "\n",
    "pill_transf = get_pil_transform()\n",
    "preprocess_transform = get_preprocess_transform()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65834c1",
   "metadata": {},
   "source": [
    "`pill_transf` is a transformer based on `get_pil_transform` that takes an image and performs standard resizing and center cropping for imagenet-based models. Obviously you can change the transformer based on whatever model use are using as high-resolution models have gained popularity in recent years.\n",
    "\n",
    "*`pill_transf` is used by the original model $f$ to make predictions on the original image.*\n",
    "\n",
    "`preprocess_transform` is another transformer that basically normalizes the image based on a pre-defined mean and variance per dimension. \n",
    "\n",
    "*`preprocess_transform` is used to generate an instance explainer where the perturbation process is applied and the explainer model is trained.*\n",
    "\n",
    "Given an image `pill_transf` returns tensors of the shape (3, 224, 224), which is in channel first layout or BCHW (Batch, Channel, Height, Width and as we are processing only a single image $x$, B=0 and is omitted). If you are using frameworks that require channel last or BHWC, then you need to transpose the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61282457",
   "metadata": {},
   "source": [
    "### Predictor\n",
    "As we have seen we are using inception-v3 and resnet-152, respectively denoted by model1 and model2 in this example.\n",
    "\n",
    "The following function `batch_predictor`, simply uses the model to make a prediction. The predictor function is later passed to `lime.lime_image.LimeImageExplainer()` in order to make predictions on the perturbed data.\n",
    "\n",
    "```python\n",
    "def batch_predict1(images):\n",
    "    model1.eval()\n",
    "    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model1.to(device)\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    logits = model1(batch)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs.detach().cpu().numpy()\n",
    "```\n",
    "This function simply take a list of images and perform prediction using the original model. This specific function uses our `model`, which we defined earlier as `model1 = models.inception_v3(pretrained=True)`\n",
    ". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e35b9",
   "metadata": {},
   "source": [
    "### Generating the local explainer\n",
    "`explainer` is defined as a `LimeImageExplainer`. As seen in the LIME paper, for categorical features, `LimeImageExplainer` perturbs the data by sampling according to the training distribution, and making a binary\n",
    "feature that is 1 when the value is the same as the instance being explained. Kernel width can be passed to this function as a parameter.  \n",
    "\n",
    "As we are performing a local explanation, we call `explain_instance` method of our `explainer` and pass the original image passed through `pill_transf` along with other parameters amongst them most notably is `num_samples`, which as the name indicates, defines the number of perturbed samples. \n",
    "\n",
    "`explain_instance` returns a `lime.lime_image.ImageExplanation` object that contains data about explanations. Most notably it includes segmentation information `ImageExplanation.segment` and make information `ImageExplanation.get_image_and_mask()`\n",
    "\n",
    "```python\n",
    "from lime import lime_image\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanations1 = []\n",
    "explanations2 = []\n",
    "for image in img:\n",
    "    explanations1.append(explainer.explain_instance(np.array(pill_transf(image)), \n",
    "                                         batch_predict1, \n",
    "                                         top_labels=5, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000) )\n",
    "    explanations2.append(explainer.explain_instance(np.array(pill_transf(image)), \n",
    "                                         batch_predict2, # classification function\n",
    "                                         top_labels=5, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000) )\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "At the next step we use `ImageExplanation.segment` to visualize segmentation information as a way of explaining which regions of image were used by the predictor model to determine the classification probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c4e64",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Finally we use skimage to visualize the segmentation we have acquired through `ImageExplanation.get_image_and_mask().`\n",
    "\n",
    "```python\n",
    "from skimage.segmentation import mark_boundaries\n",
    "ib = []\n",
    "for explanation in explanations1:\n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
    "    ib.append(mark_boundaries(temp/255.0, mask))\n",
    "fig = plt.figure(figsize=(15., 15.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(1, 3),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, ib):\n",
    "    ax.imshow(im)\n",
    "```\n",
    "\n",
    "This is only the code sample for model1-e.ginception-v3. You will find in the accompanying code notebook, that we have performed the computation for both models. Next we can see the result of visualization for all three images and both models. It is notable that the segmentation includes the environment and not just the object itself. This can explain why completely different looking savanna animals were included in the top5 selection. This, however is a hypothesis as trying the model with other animals such as gazelle could clarify the role of background, environment, and climate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb69f95",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"data/inceptionexplain.png\" alt='predictions: inception v3'  style=\"float:left\"/>\n",
    "    <figcaption>Figure 10: Segmentation generated by LIME. The regions in this segmentation are used to determine the classification of the object in the image. The underlying model in this case is inceptino-v3 and LIME has performed as a model agnostic post-hoc local explainer. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277845b4",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"data/resnetexplain.png\" alt='predictions: resnet-512'  style=\"float:left\"/>\n",
    "    <figcaption>Figure 11: Segmentation generated by LIME. The regions in this segmentation are used to determine the classification of the object in the image. The underlying model in this case is resnet-512 and LIME has performed as a model agnostic post-hoc local explainer. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9507bb2",
   "metadata": {},
   "source": [
    "# SHAP and Explainability\n",
    "Feature attribution is one of the main areas of focus in explainability research and implementation. Feature attribution is essence calculates how much each input feature contributes to a decision made by the model. Self evidently, feature attribution models can be applied post-hoc and as a black-box explainer. Prominent amongst feature attribution methods are those based on Shapley values, proposed in 1952 by Nobel Prize laureate (2012) Lloyd Shapley in game theory in the context of cooperative n-person games. To understand application of Shapley values in model explanation, let us first take a brief look at n-person cooperative games and Shapley method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b121c",
   "metadata": {},
   "source": [
    "## Cooperative n-person games\n",
    "Fundamentally, cooperative games measure how forming coalitions can help players in a competitive situation. The goal is to calculate each person's contribution to the success of the collective. The problem is to measure what contribution each player makes. Let's think in terms of a football (soccer) team. The aim of the game is to score, perhaps as many as you can if you are a proper team. Imagine a team would like to reward the whole team a certain reward based on the position in the league after the end of a season. team as well as club employees such as kit man, coaching, and medical staff based on their contribution. Obviously no one thinks that only goal scorers should be rewarded. The problem is to computer what impact each player has had in success of the team.\n",
    "Another example is a sales organization within a IT vendor company. There are account managers, solutions architects, inside sales, technical account managers, and many other in the account team as well as specialist support teams that contribute to a sale. How should the commission be distributed? What is the exact contribution of a solution architect to the success of a transaction? They story goes. What is the direct contribution of marketing to the total revenue? \n",
    "These are all n-player games in competitive situations. There is a total amount of available commission to be distributed amongst the account team and the team members, although in a tight alliance, compete over having a bigger chunk of the available rewards. Marketing, PR, and other support teams compete over the amount of available budget based on their contribution, even though in the grand scheme of things are tightly allied for the benefit of the company. \n",
    "Lloyd shapley proposed a solution to this problem. In the next section, I will introduce Shapley values. They are relevant to model explanation as we can use Shapley values to measure how much each feature contributes to the final decision of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13122d69",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "#### Characteristic Function\n",
    "Imagine a sales team. if the team has $N$ members and their coalition has a payout. Now if we create permutations of team members by changing the order or removing one or more members from the team, we still would have a payout that is either the same or lower, or perhaps, unlikely as it might be,higher. \n",
    "Let's imaging we have three players $A, B,$ and $C$. The coalition set in this case is $\\{\\{\\} = \\phi, \\{A\\}, \\{B\\}, \\{C\\}, \\{A,B\\}, \\{A,C\\}, \\{B,C\\}, \\{A,B,C\\}=\\Omega \\}$. We can see that for a coalition of $3$ we have $2^3$ permutations with each permutation resulting in some payout. For N players, there will be \n",
    "The total number of coalitions for $N$ players we have $2^N$ coalitions and each coalition has a payout. A grand coalition is a coalition that includes all the player. In this example $\\{A,B,C\\}$ is the grand coalition. We define characteristic or coalition function as a function that projects each of the coalitions to a payout. More formally:\n",
    "\n",
    "Let $N=\\{p_1, p_2, \\dots, p_n\\}$ be all the players in a game $G$ with $|N|=[N]$. Each non empty $S \\subseteq N$ is called a coalition. A grand coalition is a coalition of all players in the game and is denoted by $\\Omega$.\n",
    "\n",
    "***Definition:*** *A characteristic function game $G$* is given by $(N,v)$ where $v:2^{[N]} \\rightarrow \\mathbb{R}$ is a characteristic function that maps every $S_i \\in S$ (all coalitions + empty subset to a payout. \n",
    "\n",
    "Let us consider the sales example where A is the account manager, B the solutions architect, and C the technical account manager. The below table is the coalition function with the hypothetical payout values:\n",
    "\n",
    "| Coalition | Payout |\n",
    "|-----------|--------|\n",
    "| $\\phi$    | 0    |\n",
    "| A         | 100K   |\n",
    "| B         | 80K    |\n",
    "| C         | 65K    |\n",
    "| A,B       | 160K   |\n",
    "| A,C       | 140K   |\n",
    "| B,C       | 100K   |\n",
    "| A,B,C     | 250K   |\n",
    "\n",
    "Let us contemplate as what the payout function is telling us. \n",
    "<figure>\n",
    "    <img img align=\"right\" width=\"200\" height=\"200\" src='data/venngame.png'>\n",
    "    <figcaption>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "- A has the highest impact on this specific sale, followed by B, and then C. \n",
    "- A and B together can make a sale with a total value of 180. C adds 70 to the total. \n",
    "- B individually makes a total revenue of 80 and C adds only a contribution of 20, a far cry form individual contribution of 65. This does not seem to be a good coalition. It even makes causal sense as the two technical resources have many areas of overlap in terms of skillset.\n",
    "- C makes a total contribution of 40 when paired with A. It is a much better contribution, but the maximum effectiveness comes where all members of the coalition are working together.\n",
    "\n",
    "We can conclude C is most successful when A is in the coalition.\n",
    "\n",
    "#### Superadditive\n",
    "We all have heard of teams whose total is more that total sum of its members. This is basically intuition for the superadditive characteristic function game. More accurately we have for each combination of distinct coalitions, if the total payout of the combined coalition is more than or equal the total sum of its coalition members. The distinctness does make sense. Let us consider the case of $\\{C_A=\\{A\\}, C_{AB=}\\{A,B\\}\\}$. There is a hidden double counting of contribution of $A$ so we should account for it. In the case of our example $v(C_A)=100,v(C_B)=80,\\ v(C_{AB})=160;\\ 160=v(C_{A,B}) < v(C_A) + v(C_B) = 180$ does not contradict super additiveness. How about $v(C_{AB})$ and $v(C_C)$ where the two coalitions are distinct? $v(C_{AB}) = 160,\\ v(C_C) = 65, and  v_{ABC}=250;\\ 225 = v(C_{AB}) + v(C_B) \\leq v(C_{ABC}) = 250$ does not contradict superadditiveness.\n",
    "\n",
    "***Definition*** A characteristic function game $G=(N,v)$ is called *super additive* when:\n",
    "$$\n",
    "\\large{\\forall\\ C_i\\subseteq S,\\ where\\ \\cap_{i=1}^{[N]}=\\phi :\\   v(\\cup_{i=1}^{[N]}C_i) \\geq \\sum_{i=1}^{[N]}v(c_i)}\n",
    "$$\n",
    " \n",
    "***Question:*** *Is the sale example a super additive characteristic game function?*\n",
    "\n",
    "***Answer:*** We need to try an example that breaks superadditiveness. The case of $\\{A, C\\}$ results in $165 = v(C_A) + v(C_C) > v(C_{AC}) = 140$, therefore the sales example is not a superadditive game. Altering payouts to $v(C_{AC} \\geq 165$ will make the game to be an example of a superadditive game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a87b4f",
   "metadata": {},
   "source": [
    "A solution to a characteristic function game is a payoff vector that divides payouts amongst players in the grand coalition. More formally Shapley value is the vector $\\lambda \\in \\mathbb{R}_{\\geq 0}^{[N]}$ that satisfies:\n",
    "$$\n",
    "\\large{ \\sum_{i=1}^{[N]} \\lambda_i = v(\\Omega) }\n",
    "$$\n",
    "\n",
    "One obvious solution to the problem is of course equal payout, but this is unfair to the high performing players. A possible solution to our example is to choose $\\lambda = [\\frac{100}{250}, \\frac{80}{250}, \\frac{65}{250}]$. This means we consider impact only on the basis of individual performance. This is perhaps unfair to $C$ as he seems to excel in team settings where $A$ is present.\n",
    "\n",
    "The goal of Shapley values is to find the fair distribution in relation to impact. This requires us to define a few concept that underline fairness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3f5027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fafe69f3",
   "metadata": {},
   "source": [
    "\n",
    "# References\n",
    "- lime:https://arxiv.org/pdf/1602.04938v1.pdf\n",
    "- survey: https://arxiv.org/pdf/2011.07876.pdf\n",
    "- https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/\n",
    "- https://github.com/marcotcr/lime/tree/master/doc/notebooks\n",
    "- https://arxiv.org/pdf/1705.07874.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94547dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn2, venn2_circles, venn2_unweighted\n",
    "from matplotlib_venn import venn3, venn3_circles\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83505b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib_venn._common.VennDiagram at 0x7f88cd8f87c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "venn3(subsets = (100, 80, 160, 65, 140, 100, 250), set_labels = ('A', 'B', 'C'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16550bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4, 0.32, 0.224)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100/250, 80/250, 56/250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98755b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
